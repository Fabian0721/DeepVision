import numpy as npfrom ipython_genutils.py3compat import xrangeimport matplotlib.pyplot as pltdef sigmoid(x):    return 1/(1 + np.exp(-x))def derivative_sigmoid(x):    return sigmoid(x) * (1-sigmoid(x))def rosenbrock_function(point):    x_1, x_2 = point[0], point[1]    return 100.0 * (x_2 - x_1**2.0)**2.0 + (1 - x_1)**2.0def gradient_rosenbrock(point):    """    :param point: vector point = [x_1, x_2]^t    :return: gradient of Rosenbrock's function as np array    """    x_1, x_2 = point[0], point[1]    dx_1 = -2.0 * (1 - x_1) - 400.0 * x_1 * (x_2 - (x_1**2.0))    dx_2 = 200.0 * (x_2 - (x_1**2.0))    gradient = np.array([dx_1, dx_2])    return gradientdef gradient_descent(x_new, lrate, num_steps):    """    Description: This function takes in an initial or previous value for x, updates it based on    steps taken via the learning rate and outputs the most minimum value of x that reaches the precision satisfaction.    :param x_new: a starting value of x that will get updated based on the learning rate    :param x_prev: the previous value of x that is getting updated to the new one    :param precision: a precision that determines the stop of the stepwise descent    :param lrate: the learning rate (size of each descent step)    :output:    1. Prints out the latest new value of x which equates to the minimum we are looking for    2. Prints out the the number of x values which equates to the number of gradient descent steps    3. Plots a first graph of the function with the gradient descent path    4. Plots a second graph of the function with a zoomed in gradient descent path in the important area    :return:    """    # Create empty lists where the updated values of x and y wil be appended during each iteration    x_list, y_list = [x_new], [rosenbrock_function(x_new)]    convergence_dist = []    while len(x_list) < num_steps:        x_prev = x_new        derivative = gradient_rosenbrock(x_prev)        x_new = x_prev + (lrate * derivative)        convergence_dist.append(abs(np.linalg.norm(x_prev - x_new)))        x_list.append(x_new)        y_list.append(rosenbrock_function(x_new))    print(convergence_dist)    print("Local minimum occurs at: " + str(x_new))    print("Number of steps: " + str(len(x_list)))def adam():    passdef task1():    fig, (ax1, ax2) = plt.subplots(nrows=1, ncols=2)    x = np.arange(-3, 3)    ax1.plot(x, sigmoid(x))    ax1.plot(x, np.maximum(0, x))    ax2.plot(x, derivative_sigmoid(x))    ax1.set_xlim(-3, 3)    ax1.set_ylim(-3, 3)    ax2.set_xlim(-3, 3)    ax2.set_ylim(-3, 3)    plt.show()def task2():    # Set learning rate    lrate = 0.0001    # Initialize a point    x = np.array([.5, 2])    # Set number of steps    num_steps = 30    gradient_descent((.5, 2), lrate, num_steps)    passdef main():    task1()    # task2()if __name__ == "__main__":    main()